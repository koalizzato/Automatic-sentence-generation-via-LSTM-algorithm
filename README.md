# Automatic-sentence-generation-via-LSTM-algorithm
This project was my bachelor's thesis project, exploring the application of Long Short-Term Memory (LSTM) networks for automatic sentence generation in Italian. The focus was on building and training a deep learning model to complete partially started sentences.

## Project Overview
### 1. Data Collection and Preprocessing
- A custom dataset of over 700 sentences about chameleons was created, with data sourced from various websites and supported by synthetic data generation using Chat-GPT.
- Sentences were processed minimally to retain grammatical correctness, avoiding heavy transformations like stop-word removal, to ensure coherent outputs.

### 2. Model Design and Training
- A Bidirectional LSTM network was implemented using Python and TensorFlow, with the following architecture:
  - Embedding layer for input representation.
  - Three Bidirectional LSTM layers, each followed by dropout layers to prevent overfitting.
  - A final Dense layer with softmax activation to predict the next word.
- The model was trained using 250 epochs, with a parameter grid used to optimize hyperparameters such as the number of neurons, dropout rates, and batch sizes.

### 3. Sentence Generation
- The trained model predicts the next word in a sequence based on prior context. Sentences can be generated by providing an initial input (e.g., "The chameleon").
- Example outputs:
  - INPUT: "The chameleon" -> OUTPUT: "The chameleon is a fascinating creature capable of adapting to its environment."
  - INPUT: "Its tongue" -> OUTPUT: "Its tongue is a specialized hunting tool, quickly     launched to catch prey."

## Results and Limitations
- Strengths:
  - The model performs well with single-word inputs, generating coherent and syntactically correct sentences.
  - Punctuation like commas and periods was effectively learned and applied in outputs.
- Challenges:
  - Limited data (700 sentences) restricted the model's ability to generalize, especially with multi-word inputs.
  - The model occasionally struggles with sentence fluency and stopping at appropriate points.
- Future Improvements:
  - Expanding the dataset and experimenting with Transformer-based models like GPT or BERT could significantly enhance results.

## Conclusion
This project highlights the potential and challenges of LSTM networks in text generation tasks. While modern architectures like Transformers outperform LSTMs in this domain, this work provides valuable insights into recurrent neural network capabilities.
